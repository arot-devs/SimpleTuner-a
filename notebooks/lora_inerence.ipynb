{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b804a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "%cd ..\n",
    "\n",
    "from helpers.models.hidream.pipeline import HiDreamImagePipeline\n",
    "from helpers.models.hidream.transformer import HiDreamImageTransformer2DModel\n",
    "from transformers import PreTrainedTokenizerFast, LlamaForCausalLM\n",
    "from lycoris import create_lycoris_from_weights\n",
    "from PIL import Image\n",
    "\n",
    "# --- Config ---\n",
    "llama_repo = \"/local/yada/models/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_id = \"HiDream-ai/HiDream-I1-Full\"\n",
    "lora_path = \"/local/yada/apps/SimpleTuner-a/output/models-hidream-run04/pytorch_lora_weights.safetensors\"\n",
    "prompt = \"a girl with silver hair, detailed background\"\n",
    "negative_prompt = \"blurry, distorted\"\n",
    "width, height = 768, 1024\n",
    "guidance_scale = 5.0\n",
    "num_inference_steps = 28\n",
    "seed = 123456\n",
    "save_dir = \"output/oneshot\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# --- Load Model ---\n",
    "tokenizer_4 = PreTrainedTokenizerFast.from_pretrained(llama_repo)\n",
    "text_encoder_4 = LlamaForCausalLM.from_pretrained(llama_repo, output_hidden_states=True, torch_dtype=torch.bfloat16)\n",
    "transformer = HiDreamImageTransformer2DModel.from_pretrained(model_id, torch_dtype=torch.bfloat16, subfolder=\"transformer\")\n",
    "\n",
    "pipeline = HiDreamImagePipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    tokenizer_4=tokenizer_4,\n",
    "    text_encoder_4=text_encoder_4,\n",
    "    transformer=transformer,\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "pipeline.to(device)\n",
    "\n",
    "# --- Apply LoRA ---\n",
    "wrapper, _ = create_lycoris_from_weights(1.0, lora_path, pipeline.transformer)\n",
    "wrapper.merge_to()\n",
    "\n",
    "# --- Encode Prompt ---\n",
    "t5_embeds, llama_embeds, negative_t5_embeds, negative_llama_embeds, pooled_embeds, negative_pooled_embeds = pipeline.encode_prompt(\n",
    "    prompt=prompt, prompt_2=prompt, prompt_3=prompt, prompt_4=prompt, num_images_per_prompt=1\n",
    ")\n",
    "\n",
    "# --- Generate ---\n",
    "image = pipeline(\n",
    "    t5_prompt_embeds=t5_embeds,\n",
    "    llama_prompt_embeds=llama_embeds,\n",
    "    pooled_prompt_embeds=pooled_embeds,\n",
    "    negative_t5_prompt_embeds=negative_t5_embeds,\n",
    "    negative_llama_prompt_embeds=negative_llama_embeds,\n",
    "    negative_pooled_prompt_embeds=negative_pooled_embeds,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    generator=torch.Generator(device=device).manual_seed(seed),\n",
    "    width=width,\n",
    "    height=height,\n",
    "    guidance_scale=guidance_scale,\n",
    ").images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af794cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\u001b[1mModel Key                   | Description\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[94maesthetic-siglip           \u001b[0m | \u001b[90mA Siglip-based aesthetic model for high-efficiency aesthetic predictions.     输入anime图片, 输出预测的siglip aesthetic score         https://github.com/discus0434/ae...\u001b[0m\n",
      "\u001b[94maesthetic-pixiv-compound   \u001b[0m | \u001b[90mAesthetic model trained on pixiv data (of the constructed pixiv compound aesthetic score)     model at \"https://bucket-public-access-uw2.s3.us-west-2.amazona...\u001b[0m\n",
      "\u001b[94maesthetic-clip             \u001b[0m | \u001b[90mCalculate and cache clip aesthetics embs and calcualtes the aesthetic scores and 0shot cls scores\u001b[0m\n",
      "\u001b[94maesthetic-pixai-1.1        \u001b[0m | \u001b[90mAesthetic model trained on pixiv data (of the constructed pixiv compound aesthetic score)     model at \"https://bucket-public-access-uw2.s3.us-west-2.amazona...\u001b[0m\n",
      "\u001b[94maesthetic-pixai-1.1-anatomy\u001b[0m | \u001b[90mAesthetic model trained on pixiv data (of the constructed pixiv compound aesthetic score)     model at \"https://bucket-public-access-uw2.s3.us-west-2.amazona...\u001b[0m\n",
      "\u001b[94mdistill-q-align-aesthetic  \u001b[0m | \u001b[90mA model trained for predicting image aesthetics using QAlign.     输入图片, 输出图片美学评分      Params:         device: \"cuda\" or \"cpu\".         batch_size: Batch size...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from procslib import get_model, print_model_keys\n",
    "\n",
    "print_model_keys(\"aes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simpletuner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
